{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKkIMRSIRh1tWYmWX3XCg1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lathadevi158/Learning_AI_Sandbox/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------**NLP**-------------------"
      ],
      "metadata": {
        "id": "qbrrum2DS9yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP stands for Natural Language Processing. In simple terms, it‚Äôs a field of Artificial Intelligence (AI) that focuses on making computers understand, interpret, and generate human language\n",
        "\n",
        "Examples you use every day:\n",
        "\n",
        "ChatGPT ‚Üí answering questions\n",
        "\n",
        "Google Translate ‚Üí translating languages\n",
        "\n",
        "Spam filters ‚Üí detecting unwanted emails\n",
        "\n",
        "\n",
        "NLP Pipeline (Simplified)\n",
        "\n",
        "Text Input ‚Üí raw text from user/documents\n",
        "\n",
        "Preprocessing ‚Üí clean and structure the text\n",
        "\n",
        "Feature Extraction ‚Üí convert text to numbers (vectors)\n",
        "\n",
        "Modeling / Understanding ‚Üí ML or LLM predicts/generates output\n",
        "\n",
        "Output Generation ‚Üí human-readable answer/text"
      ],
      "metadata": {
        "id": "SkkYC4bbTxkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3Ô∏è‚É£ Text Preprocessing Techniques\n",
        "\n",
        "Before feeding text into a model, we clean it:\n",
        "\n",
        "Technique\tPurpose\tPython Example\n",
        "Tokenization\tSplit text into words or subwords\t\"I love AI\" ‚Üí [\"I\", \"love\", \"AI\"]\n",
        "Lowercasing\tStandardize text\t\"AI is Cool\" ‚Üí \"ai is cool\"\n",
        "Stopword Removal\tRemove common words that add little meaning\t\"I love AI\" ‚Üí [\"love\", \"AI\"]\n",
        "Stemming\tReduce words to base form (crude)\t\"running\" ‚Üí \"run\"\n",
        "Lemmatization\tReduce words to dictionary form (better)\t\"running\" ‚Üí \"run\"\n",
        "\n",
        "4Ô∏è‚É£ Representing Text as Numbers\n",
        "\n",
        "Models can‚Äôt understand raw text, so we convert it into vectors:\n",
        "\n",
        "Bag of Words (BoW)\n",
        "\n",
        "Count how many times each word appears\n",
        "\n",
        "Example: \"I love AI\", \"AI is fun\" ‚Üí vector representation\n",
        "\n",
        "TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)\n",
        "\n",
        "Gives importance to rare words\n",
        "\n",
        "Example: common words like ‚Äúis‚Äù get less weight\n",
        "\n",
        "This is how LLMs and classical NLP understand meaning from text.\n"
      ],
      "metadata": {
        "id": "92e5LXNmS9LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = \"I love exploring Generative AI with ChatGPT!\"\n",
        "\n",
        "# 1. Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# 2. Stopword removal\n",
        "filtered_tokens = [t for t in tokens if t.lower() not in stopwords.words('english')]\n",
        "print(\"Without stopwords:\", filtered_tokens)\n",
        "\n",
        "# 3. Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(t) for t in filtered_tokens]\n",
        "print(\"Stemmed:\", stemmed)\n",
        "\n",
        "# 4. Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
        "print(\"Lemmatized:\", lemmatized)\n",
        "\n"
      ],
      "metadata": {
        "id": "cvtJAN1wTALp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726bf4d1-464f-4cc0-d313-7a1280d84b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['I', 'love', 'exploring', 'Generative', 'AI', 'with', 'ChatGPT', '!']\n",
            "Without stopwords: ['love', 'exploring', 'Generative', 'AI', 'ChatGPT', '!']\n",
            "Stemmed: ['love', 'explor', 'gener', 'ai', 'chatgpt', '!']\n",
            "Lemmatized: ['love', 'exploring', 'Generative', 'AI', 'ChatGPT', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Assuming 'documents' is a list of strings (your text data)\n",
        "# Replace this with your actual data if needed\n",
        "documents = [\"I love AI\", \"AI is fun\"]\n",
        "\n",
        "# 1Ô∏è‚É£ Bag of Words (BoW)\n",
        "# -----------------------------\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow = bow_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Bag of Words Vocabulary:\")\n",
        "print(bow_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBag of Words Vectors:\")\n",
        "print(bow.toarray())\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ TF-IDF\n",
        "# -----------------------------\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"\\nTF-IDF Vocabulary:\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nTF-IDF Vectors:\")\n",
        "print(tfidf.toarray())"
      ],
      "metadata": {
        "id": "GM81S8bAUCnM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1667c8c0-7963-41c0-9125-3e8314cf624f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Vocabulary:\n",
            "['ai' 'fun' 'is' 'love']\n",
            "\n",
            "Bag of Words Vectors:\n",
            "[[1 0 0 1]\n",
            " [1 1 1 0]]\n",
            "\n",
            "TF-IDF Vocabulary:\n",
            "['ai' 'fun' 'is' 'love']\n",
            "\n",
            "TF-IDF Vectors:\n",
            "[[0.57973867 0.         0.         0.81480247]\n",
            " [0.44943642 0.6316672  0.6316672  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üçï Stemming vs Lemmatization ‚Äì The Food Analogy\n",
        "\n",
        "Imagine words are like pizzas üçï\n",
        "\n",
        "Technique\tAnalogy\tExample\n",
        "Stemming\tYou take a pizza and just bite off a big chunk. Doesn‚Äôt matter if it‚Äôs messy, you just want the main part.\t\"running\" ‚Üí \"run\" ; \"flies\" ‚Üí \"fli\" (oops, some weird bite!)\n",
        "\n",
        "Lemmatization\tYou carefully cut the pizza into neat slices so it looks perfect and edible.\t\"running\" ‚Üí \"run\" ; \"flies\" ‚Üí \"fly\" (looks exactly like it should!)\n",
        "\n",
        "Stemming = messy but fast snack bite üçΩÔ∏è\n",
        "\n",
        "Lemmatization = neat, proper slice üçï‚ú®"
      ],
      "metadata": {
        "id": "KPpSY8ebURl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ Parts of Speech (POS) Tagging\n",
        "\n",
        "What it is:\n",
        "\n",
        "Every word in a sentence has a role: noun, verb, adjective, etc.\n",
        "\n",
        "POS tagging tells the computer what role each word plays.\n",
        "\n",
        "Kid/Food Analogy:\n",
        "\n",
        "Imagine a sentence is a kitchen:\n",
        "\n",
        "Nouns = ingredients (apple, sugar)\n",
        "\n",
        "Verbs = actions (chop, mix)\n",
        "\n",
        "Adjectives = descriptions (sweet, fresh)\n",
        "\n",
        "Example Sentence:\n",
        "\"I love eating chocolate\"\n",
        "\n",
        "I ‚Üí Pronoun\n",
        "\n",
        "love ‚Üí Verb\n",
        "\n",
        "eating ‚Üí Verb\n",
        "\n",
        "chocolate ‚Üí Noun"
      ],
      "metadata": {
        "id": "unY4RmW8VOjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"I love eating chocolate\"\n",
        "\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHPkkbLVVFpz",
        "outputId": "8d987bc4-5f06-4b16-837c-ce384ba085ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON\n",
            "love VERB\n",
            "eating VERB\n",
            "chocolate NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2Ô∏è‚É£ Named Entity Recognition (NER)\n",
        "\n",
        "What it is:\n",
        "\n",
        "Finds special entities in text: names, places, dates, organizations.\n",
        "\n",
        "Kid/Food Analogy:\n",
        "\n",
        "Imagine you‚Äôre picking special ingredients from a big bowl:\n",
        "\n",
        "‚ÄúElon Musk founded SpaceX in 2002‚Äù ‚Üí\n",
        "\n",
        "Elon Musk = PERSON\n",
        "\n",
        "SpaceX = ORG\n",
        "\n",
        "2002 = DATE"
      ],
      "metadata": {
        "id": "m4fsCd7bVbp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "id": "QUppvCKcVbYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3Ô∏è‚É£ Sentiment Analysis\n",
        "\n",
        "What it is:\n",
        "\n",
        "Detects emotion or opinion in text: positive, negative, neutral.\n",
        "\n",
        "Kid/Food Analogy:\n",
        "\n",
        "Taste test your food:\n",
        "\n",
        "\"I love pizza!\" ‚Üí üòä Positive\n",
        "\n",
        "\"This soup is terrible.\" ‚Üí üòû Negative"
      ],
      "metadata": {
        "id": "H_dbvvQ1VbBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"I love pizza but hate onions\"\n",
        "blob = TextBlob(text)\n",
        "print(blob.sentiment)  # polarity, subjectivity\n",
        "\n",
        "#Polarity > 0 ‚Üí positive, <0 ‚Üí negative, 0 ‚Üí neutral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fwT2v1zVaw1",
        "outputId": "54f98451-1271-420c-bb70-518f52085b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=-0.15000000000000002, subjectivity=0.75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4Ô∏è‚É£ Word Embeddings (Word2Vec / GloVe)\n",
        "\n",
        "What it is:\n",
        "\n",
        "Converts words into numbers (vectors) so models can see similarity and meaning.\n",
        "\n",
        "Words with similar meaning have vectors close to each other.\n",
        "\n",
        "Kid/Food Analogy:\n",
        "\n",
        "Imagine a fridge map:\n",
        "\n",
        "‚Äúapple‚Äù is near ‚Äúbanana‚Äù (both fruits)\n",
        "\n",
        "‚Äúcarrot‚Äù is a bit farther (vegetable)\n",
        "\n",
        "So the computer knows which words are ‚Äúsimilar‚Äù in meaning."
      ],
      "metadata": {
        "id": "xwnd9ZHcVacA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = [[\"I\", \"love\", \"pizza\"], [\"Pizza\", \"is\", \"delicious\"], [\"I\", \"hate\", \"onions\"]]\n",
        "model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, workers=1)\n",
        "\n",
        "print(model.wv['pizza'])  # vector representation of \"pizza\"\n",
        "print(model.wv.similarity('pizza', 'delicious'))  # similarity score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiT5r7scVaLl",
        "outputId": "5c97be9b-4625-43c7-bd78-d665721d2943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.01577653  0.00321372 -0.0414063  -0.07682689 -0.01508008  0.02469795\n",
            " -0.00888027  0.05533662 -0.02742977  0.02260065]\n",
            "-0.11387499\n"
          ]
        }
      ]
    }
  ]
}